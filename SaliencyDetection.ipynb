{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers,optimizers\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Deconvolution2D, Cropping2D\n",
    "from keras.layers import Input, Add, Dropout, Permute, add\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger,EarlyStopping\n",
    "from scipy.misc import toimage, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_detection():\n",
    "    vgg_16_model = VGG16(weights=None,include_top = False)\n",
    "     \n",
    "    #get weights from pretrained model on imagenet\n",
    "    #vgg_16_model.load_weights('vgg_weights.h5')\n",
    "    \n",
    "    inputs = Input(shape = (224,224,3), name = \"image_input\")\n",
    "    \n",
    "    #create dummy layer\n",
    "    output_vgg16_model = vgg_16_model(inputs)\n",
    "\n",
    "    #Making the network fully convolutional \n",
    "    conv1_7x7 = Convolution2D(4096,kernel_size=(7,7),padding = \"same\",activation = \"relu\",name = \"FullConv1\")(output_vgg16_model)\n",
    "    dropout_1 = Dropout(0.2)(conv1_7x7)\n",
    "    conv2_1x1 = Convolution2D(4096,kernel_size=(1,1),padding = \"same\",activation = \"relu\",name = \"FullConv2\")(dropout_1)\n",
    "    dropout_2 = Dropout(0.2)(conv2_1x1)\n",
    "    \n",
    "    conv3_1x1 = Convolution2D(1,kernel_size=(1,1),padding = \"same\",activation = \"relu\",name = \"FullConv3\")(dropout_2)\n",
    "    deconv1_1x1 = Deconvolution2D(1,kernel_size=(1,1), strides= (32,32),padding = \"valid\",activation='softmax',name = \"Deconv1\")(conv3_1x1)\n",
    "    \n",
    "    return Model(inputs = inputs, outputs = deconv1_1x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  saliency_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "FullConv1 (Conv2D)           (None, 7, 7, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 4096)        0         \n",
      "_________________________________________________________________\n",
      "FullConv2 (Conv2D)           (None, 7, 7, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 4096)        0         \n",
      "_________________________________________________________________\n",
      "FullConv3 (Conv2D)           (None, 7, 7, 1)           4097      \n",
      "_________________________________________________________________\n",
      "Deconv1 (Conv2DTranspose)    (None, 224, 224, 1)       2         \n",
      "=================================================================\n",
      "Total params: 134,264,643\n",
      "Trainable params: 134,264,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining Callback functions which will be called by model during runtime when specified condition satisfies\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)\n",
    "csv_logger = CSVLogger('./vgg16_saliency.csv')\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error',optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.load('train_images_saliency.npy')\n",
    "train_labels = np.load('train_labels_saliency.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_augmentation :\n",
    "    print(\"-------------Using Data augmentation------------\")\n",
    "     # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    \n",
    "    datagen.fit(x_train)\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,verbose=1,validation_data=(x_test,y_test),callbacks=[lr_reducer, early_stopper, csv_logger])\n",
    "    \n",
    "else :\n",
    "    print(\"-----Not Using Data augmentation---------------\")\n",
    "    model.fit(train_images, train_labels,\n",
    "              batch_size=7,\n",
    "              epochs=20,\n",
    "              validation_split= 0.3,\n",
    "              shuffle=True,callbacks=[lr_reducer, early_stopper, csv_logger])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
